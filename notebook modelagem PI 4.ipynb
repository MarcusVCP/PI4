{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d31d4b",
   "metadata": {},
   "source": [
    "# Modelagem de Dados — Previsão de Diabetes (Pima Indians Diabetes Dataset)\n",
    "\n",
    "Este notebook contém o pipeline completo de modelagem para o Projeto Integrador:\n",
    "- Carregamento do dataset\n",
    "- Pré-processamento e limpeza\n",
    "- Análise exploratória básica\n",
    "- Treinamento e avaliação de modelos (Random Forest, Regressão Logística)\n",
    "- Visualizações (matriz de confusão, curva ROC, importância das variáveis)\n",
    "- Salvamento do modelo\n",
    "\n",
    "> Observação: o dataset pode ser baixado diretamente do link público ou usado um arquivo local chamado `pima-indians-diabetes.data.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importar bibliotecas e carregar o dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Caminhos de arquivo\n",
    "csv_path_local = '/mnt/data/pima-indians-diabetes.data.csv'\n",
    "csv_path_save = '/mnt/data/pima-indians-diabetes.csv'\n",
    "\n",
    "# Colunas esperadas (o arquivo original não tem header)\n",
    "cols = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\n",
    "\n",
    "# Tentar carregar: primeiro local, senão tentar ler a URL (se disponível)\n",
    "if os.path.exists(csv_path_local):\n",
    "    df = pd.read_csv(csv_path_local, header=None, names=cols)\n",
    "elif os.path.exists(csv_path_save):\n",
    "    df = pd.read_csv(csv_path_save)\n",
    "else:\n",
    "    try:\n",
    "        url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "        df = pd.read_csv(url, header=None, names=cols)\n",
    "        df.to_csv(csv_path_save, index=False)\n",
    "        print('Dataset baixado e salvo em', csv_path_save)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError('Nenhum dataset local encontrado e download falhou. Faça upload do CSV para /mnt/data/pima-indians-diabetes.data.csv') from e\n",
    "\n",
    "print('Formato do dataset:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Pré-processamento e limpeza\n",
    "df_orig = df.copy()\n",
    "\n",
    "# Algumas colunas usam 0 como valor ausente (Glucose, BloodPressure, SkinThickness, Insulin, BMI)\n",
    "cols_zero_na = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "\n",
    "# Substituir zeros por NaN nessas colunas\n",
    "df[cols_zero_na] = df[cols_zero_na].replace(0, np.nan)\n",
    "\n",
    "# Estatísticas antes do tratamento\n",
    "missing_before = df.isna().sum()\n",
    "\n",
    "# Imputação simples com mediana para manter robustez\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[cols_zero_na] = imputer.fit_transform(df[cols_zero_na])\n",
    "\n",
    "# Conferir depois\n",
    "missing_after = df.isna().sum()\n",
    "\n",
    "print('Valores ausentes antes:\\n', missing_before)\n",
    "print('Valores ausentes depois:\\n', missing_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Análise Exploratória básica\n",
    "# Estatísticas descritivas\n",
    "desc = df.describe()\n",
    "\n",
    "# Contagem por classe\n",
    "class_counts = df['Outcome'].value_counts()\n",
    "\n",
    "# Correlação com target\n",
    "corr_target = df.corr()['Outcome'].sort_values(ascending=False)\n",
    "\n",
    "print('Descrição:\n",
    "', desc)\n",
    "print('\\nContagem por classe:\\n', class_counts)\n",
    "print('\\nCorrelação com Outcome:\\n', corr_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17786d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dividir em treino e teste e padronizar\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Padronizar features numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Treinar modelos: Random Forest (base) e Regressão Logística\n",
    "# Random Forest inicial\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=200)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Regressão logística para comparação\n",
    "log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log.fit(X_train_scaled, y_train)\n",
    "y_pred_log = log.predict(X_test_scaled)\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "print('Acurácia RF:', acc_rf)\n",
    "print('Acurácia LogReg:', acc_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Avaliação detalhada do Random Forest (modelo escolhido)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Classification Report (Random Forest):\\n', classification_report(y_test, y_pred_rf))\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print('\\nMatriz de Confusão:\\n', cm)\n",
    "\n",
    "# Curva ROC e AUC\n",
    "y_proba_rf = rf.predict_proba(X_test_scaled)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_rf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('\\nAUC ROC:', roc_auc)\n",
    "\n",
    "# Plotar matriz de confusão (matplotlib, sem paleta de cores explícita)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Real')\n",
    "for (i,j),val in np.ndenumerate(cm):\n",
    "    plt.text(j, i, val, ha='center', va='center')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.title(f'Curva ROC (AUC = {roc_auc:.3f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Importância das variáveis (Random Forest)\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importances)\n",
    "# Plot simples\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(importances['feature'], importances['importance'])\n",
    "plt.title('Importância das variáveis (Random Forest)')\n",
    "plt.xlabel('Importância')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84326d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Salvar modelo, scaler e resultados de teste\n",
    "model_path = '/mnt/data/diabetes_model_rf.pkl'\n",
    "scaler_path = '/mnt/data/diabetes_scaler.pkl'\n",
    "predictions_path = '/mnt/data/predictions_test.csv'\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Salvar DataFrame com previsões de teste\n",
    "df_test = X_test.copy()\n",
    "df_test['y_true'] = y_test.values\n",
    "df_test['y_pred'] = y_pred_rf\n",
    "df_test['y_proba'] = y_proba_rf\n",
    "df_test.to_csv(predictions_path, index=False)\n",
    "\n",
    "print('Modelos e resultados salvos em:')\n",
    "print('-', model_path)\n",
    "print('-', scaler_path)\n",
    "print('-', predictions_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566ffb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Observações finais:**\n",
    "\n",
    "- O próximo passo será construir o dashboard no Power BI utilizando `predictions_test.csv` e, se necessário, incorporar o script Python para geração das previsões diretamente no Power BI. \n",
    "\n",
    "- Recomendações: executar validação cruzada, ajuste de hiperparâmetros com GridSearchCV, e priorizar o ajuste de recall caso o custo de falsos negativos seja alto no contexto clínico.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
