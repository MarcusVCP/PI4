{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9qm0Uo834EG"
      },
      "source": [
        "# Projeto Integrador 4 - UNIVESP\n",
        "## Análise de dados para previsão de diabetes com Machine Learning\n",
        "Alunos: Marcus Vinicius Camargo Pereira | Wallace Cosme da Silva\n",
        "---"
      ],
      "id": "G9qm0Uo834EG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD-eG-i2ojK_"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 1. Instalar biblioteca necessária\n",
        "# ===============================================================\n",
        "\n",
        "!pip install gspread"
      ],
      "id": "tD-eG-i2ojK_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXjS6iEwpmxL"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 2. Importar bibliotecas necessárias e autenticar o Google\n",
        "# ===============================================================\n",
        "\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread_dataframe as gd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Autenticação do usuário\n",
        "auth.authenticate_user()\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "# Autoriza o gspread\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "print(\"Autenticação concluída com sucesso.\")"
      ],
      "id": "kXjS6iEwpmxL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3qHiD5TqA8B"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 3. Carregamento dos dados do Google Sheets (Treinamento e previsão)\n",
        "# ===============================================================\n",
        "\n",
        "NOME_DA_PLANILHA = \"Base de dados PI\"\n",
        "ABA_DADOS_ORIGINAIS = \"Dados\"\n",
        "ABA_NOVOS_DADOS = \"Respostas ao formulário 1\"\n",
        "\n",
        "df_treino = pd.DataFrame()\n",
        "df_para_prever = pd.DataFrame()\n",
        "lista_dfs_novos = []\n",
        "\n",
        "try:\n",
        "    print(f\"Abrindo a planilha: '{NOME_DA_PLANILHA}'...\")\n",
        "    planilha = gc.open(NOME_DA_PLANILHA)\n",
        "\n",
        "    # 1. Carrega os dados originais para TREINO\n",
        "    print(f\"Lendo a aba de TREINO: '{ABA_DADOS_ORIGINAIS}'...\")\n",
        "    aba_originais = planilha.worksheet(ABA_DADOS_ORIGINAIS)\n",
        "    df_treino = pd.DataFrame(aba_originais.get_all_records())\n",
        "    print(f\"Encontrados {len(df_treino)} registros para TREINO.\")\n",
        "\n",
        "    # 2. Carrega os novos dados (do Formulário) para PREVISÃO\n",
        "    try:\n",
        "        print(f\"Lendo a aba de PREVISÃO: '{ABA_NOVOS_DADOS}'...\")\n",
        "        aba_novos = planilha.worksheet(ABA_NOVOS_DADOS)\n",
        "        df_para_prever = pd.DataFrame(aba_novos.get_all_records())\n",
        "        lista_dfs_novos.append(df_para_prever)\n",
        "\n",
        "        if 'Carimbo de data/hora' in df_para_prever.columns:\n",
        "            df_para_prever = df_para_prever.drop(columns=['Carimbo de data/hora'])\n",
        "        elif 'Timestamp' in df_para_prever.columns:\n",
        "            df_para_prever = df_para_prever.drop(columns=['Timestamp'])\n",
        "\n",
        "        print(f\"Encontrados {len(df_para_prever)} novos registros para PREVISÃO.\")\n",
        "\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Aba '{ABA_NOVOS_DADOS}' não encontrada ou vazia. Nenhum dado para prever.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERRO: Não foi possível abrir a planilha. Verifique o nome '{NOME_DA_PLANILHA}' e se você deu permissão.\")\n",
        "    print(e)"
      ],
      "id": "U3qHiD5TqA8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3d_G23YqPh-"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 4. Tradução dos nomes para português\n",
        "# ===============================================================\n",
        "\n",
        "mapa_nomes = {\n",
        "    'Pregnancies': 'Gravidez',\n",
        "    'Glucose': 'Glicose',\n",
        "    'BloodPressure': 'PressaoArterial',\n",
        "    'SkinThickness': 'EspessuraPele',\n",
        "    'Insulin': 'Insulina',\n",
        "    'BMI': 'IMC',\n",
        "    'DiabetesPedigreeFunction': 'FuncaoGeneticaDiabetes',\n",
        "    'Age': 'Idade',\n",
        "    'Outcome': 'Resultado'\n",
        "}\n",
        "\n",
        "df_treino = df_treino.rename(columns=mapa_nomes)\n",
        "print(\"Colunas de TREINO traduzidas.\")\n",
        "\n",
        "\n",
        "if not df_para_prever.empty:\n",
        "    df_para_prever = df_para_prever.rename(columns=mapa_nomes)\n",
        "    print(\"Colunas de PREVISÃO traduzidas.\")"
      ],
      "id": "N3d_G23YqPh-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q11Q9mYarF9m"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 5. Limpeza de dados\n",
        "# ===============================================================\n",
        "\n",
        "cols_zero_na = ['Glicose', 'PressaoArterial', 'EspessuraPele', 'Insulina', 'IMC']\n",
        "df = df_treino.copy() # Usar 'df' como variável temporária para o treino\n",
        "\n",
        "# 1. Lidar com strings vazias (caso haja algum erro)\n",
        "df[cols_zero_na] = df[cols_zero_na].replace('', np.nan)\n",
        "df[cols_zero_na] = df[cols_zero_na].replace(' ', np.nan)\n",
        "\n",
        "# 2. Forçar colunas para tipo numérico\n",
        "for col in cols_zero_na:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 3. Substituir zeros\n",
        "print(\"Substituindo valores 0 por NaN nos dados de TREINO...\")\n",
        "df[cols_zero_na] = df[cols_zero_na].replace(0, np.nan)\n",
        "\n",
        "df_treino = df.copy() # Atualiza o df_treino principal\n",
        "print(\"\\nValores ausentes (TREINO):\")\n",
        "print(df_treino.isnull().sum())"
      ],
      "id": "q11Q9mYarF9m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-mD9g_1ri_E"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 6. Imputar dados ausentes\n",
        "# ===============================================================\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"Preparando o Imputer (calculando medianas) nos dados de TREINO...\")\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Separar as características do alvo para imputação\n",
        "# O imputador deve aprender e imputar apenas colunas de características\n",
        "X_for_imputation = df_treino.drop('Resultado', axis=1)\n",
        "y_original = df_treino['Resultado']\n",
        "\n",
        "# O Imputer é treinado (ajustado) e aplicado (transformado) apenas nas características\n",
        "X_imputado = pd.DataFrame(imputer.fit_transform(X_for_imputation), columns=X_for_imputation.columns)\n",
        "\n",
        "# Recombina as características imputadas com a variável alvo original\n",
        "df_imputado = X_imputado.copy()\n",
        "df_imputado['Resultado'] = y_original\n",
        "\n",
        "# Preservar tipos de dados originais (assuming Resultado is int)\n",
        "df_imputado['Resultado'] = df_imputado['Resultado'].astype(int)\n",
        "\n",
        "print(\"Valores ausentes após imputação (TREINO):\")\n",
        "print(df_imputado.isnull().sum())\n",
        "df_treino = df_imputado # Atualiza o df_treino"
      ],
      "id": "0-mD9g_1ri_E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67fG-k3Yrq4M"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 7. Análise Exploratória\n",
        "# ===============================================================\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(df_treino.describe().T)\n",
        "\n",
        "sns.countplot(x='Resultado', data=df_treino, palette='viridis')\n",
        "plt.title('Distribuição de Casos de Diabetes (Dados de Treino)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_treino.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Heatmap de Correlação (Dados de Treino)')\n",
        "plt.show()"
      ],
      "id": "67fG-k3Yrq4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8r16i_JsCjN"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 8. Separação dos dados de treino e previsão\n",
        "# ===============================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Divisão em features (X) e target (y) - A PARTIR DO DF_TREINO\n",
        "X = df_treino.drop('Resultado', axis=1)\n",
        "y = df_treino['Resultado']\n",
        "\n",
        "# 2. Divisão em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Formato de X_train: {X_train.shape}\")\n",
        "print(f\"Formato de X_test: {X_test.shape}\")\n",
        "\n",
        "# 3. Padronização (Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nDados de treino padronizados com sucesso.\")"
      ],
      "id": "Y8r16i_JsCjN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1g0yF0Gsa9c"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 9. Treinamento do modelo\n",
        "# ===============================================================\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pickle\n",
        "\n",
        "# --- 1. Random Forest (Modelo Escolhido) ---\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "\n",
        "print(\"Treinando o modelo Random Forest...\")\n",
        "# Usamos os dados NÃO padronizados para o RF\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"Modelo treinado com sucesso.\")\n",
        "\n",
        "# --- 2. Regressão Logística (Para Comparação) ---\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)"
      ],
      "id": "J1g0yF0Gsa9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqO3GNN6s-xP"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 10. Avaliação do modelo\n",
        "# ===============================================================\n",
        "\n",
        "# Previsões no conjunto de TESTE (do df_treino)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_proba_rf_test = rf.predict_proba(X_test)[:, 1] # Probabilidade da classe 1 (Diabetes)\n",
        "\n",
        "print(\"--- Avaliação do Random Forest (nos 25% de dados de teste) ---\")\n",
        "print(f\"Acurácia: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Matriz de Confusão\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Previsto: Não', 'Previsto: Sim'],\n",
        "            yticklabels=['Real: Não', 'Real: Sim'])\n",
        "plt.title('Matriz de Confusão - Dados de Teste')\n",
        "plt.show()"
      ],
      "id": "JqO3GNN6s-xP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv5b1XvNtP0_"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 11. Importância das variáveis\n",
        "# ===============================================================\n",
        "\n",
        "# Criar um DataFrame com a importância das features\n",
        "importancias = pd.DataFrame({\n",
        "    'Variavel': X_train.columns,\n",
        "    'Importancia': rf.feature_importances_\n",
        "})\n",
        "importancias = importancias.sort_values(by='Importancia', ascending=False)\n",
        "print(importancias)\n",
        "\n",
        "sns.barplot(x='Importancia', y='Variavel', data=importancias, palette='mako')\n",
        "plt.title('Importância das Variáveis (Random Forest)')\n",
        "plt.show()"
      ],
      "id": "Yv5b1XvNtP0_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 12. Limpeza dos novos dados\n",
        "# ===============================================================\n",
        "\n",
        "if not df_para_prever.empty:\n",
        "    print(\"Limpando os novos dados do formulário...\")\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------------\n",
        "    # 1. TRADUÇÃO DO FORMULÁRIO (DE INGLÊS PARA PORTUGUÊS)\n",
        "    # Assumimos que o formulário usa os nomes padrão do dataset Pima (em Inglês).\n",
        "    # --------------------------------------------------------------------------------------------------\n",
        "    mapa_traducao = {\n",
        "        'Pregnancies': 'Gravidez',\n",
        "        'Glucose': 'Glicose',\n",
        "        'BloodPressure': 'PressaoArterial',\n",
        "        'SkinThickness': 'EspessuraPele',\n",
        "        'Insulin': 'Insulina',\n",
        "        'BMI': 'IMC',\n",
        "        'DiabetesPedigreeFunction': 'FuncaoGeneticaDiabetes',\n",
        "        'Age': 'Idade',\n",
        "    }\n",
        "\n",
        "    # Aplica a tradução nos dados do formulário\n",
        "    df_para_prever = df_para_prever.rename(columns=mapa_traducao)\n",
        "    print(\"Colunas do formulário traduzidas de Inglês para Português.\")\n",
        "\n",
        "\n",
        "    cols_features = X_train.columns # Pega a lista ['Gravidez', 'Glicose', ...]\n",
        "    cols_zero_na = ['Glicose', 'PressaoArterial', 'EspessuraPele', 'Insulina', 'IMC']\n",
        "\n",
        "    try:\n",
        "        # Agora deve funcionar, pois os nomes foram traduzidos para PT-BR\n",
        "        X_novos = df_para_prever[cols_features].copy()\n",
        "    except KeyError as e:\n",
        "        print(\"\\n--- ERRO CRÍTICO (KeyError) NA SELEÇÃO FINAL ---\")\n",
        "        print(\"O nome da coluna ainda não está no padrão do modelo, mesmo após a tradução.\")\n",
        "        print(f\"COLUNAS ESPERADAS (Modelo): {list(cols_features)}\")\n",
        "        print(f\"COLUNAS ATUAIS (Formulário APÓS TRADUÇÃO): {list(df_para_prever.columns)}\")\n",
        "        raise e\n",
        "\n",
        "    # 1. Limpeza de strings vazias e conversão para numérico\n",
        "    X_novos[cols_zero_na] = X_novos[cols_zero_na].replace('', np.nan)\n",
        "    X_novos[cols_zero_na] = X_novos[cols_zero_na].replace(' ', np.nan)\n",
        "    for col in cols_zero_na:\n",
        "        X_novos[col] = pd.to_numeric(X_novos[col], errors='coerce')\n",
        "\n",
        "    # 2. Substituir zeros por NaN (regra de limpeza do Pima)\n",
        "    X_novos[cols_zero_na] = X_novos[cols_zero_na].replace(0, np.nan)\n",
        "\n",
        "    # 3. Imputação (Etapa 6) - Usando o imputer JÁ TREINADO\n",
        "    print(\"Aplicando imputação (medianas do treino) nos novos dados...\")\n",
        "    X_novos_imputado = pd.DataFrame(imputer.transform(X_novos), columns=X_novos.columns)\n",
        "\n",
        "    mapa_reverso = {v: k for k, v in mapa_traducao.items()}\n",
        "    X_novos_imputado_en = X_novos_imputado.rename(columns=mapa_reverso)\n",
        "\n",
        "    print(\"Limpeza e preparação dos novos dados concluída.\")\n",
        "\n",
        "else:\n",
        "    print(\"Nenhum dado novo do formulário para limpar ou prever.\")"
      ],
      "metadata": {
        "id": "OF2hiHYPZYPx"
      },
      "id": "OF2hiHYPZYPx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_code_2"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 13. Previsão dos novos dados\n",
        "# ===============================================================\n",
        "\n",
        "if not df_para_prever.empty:\n",
        "    print(\"Executando previsões nos novos pacientes...\")\n",
        "\n",
        "    # Usar o modelo 'rf' treinado nos dados de treino\n",
        "    # para prever nos novos dados imputados\n",
        "    novas_previsoes = rf.predict(X_novos_imputado)\n",
        "    novas_probabilidades = rf.predict_proba(X_novos_imputado)[:, 1]\n",
        "\n",
        "    # Adicionar as previsões de volta ao DataFrame original do formulário\n",
        "    df_para_prever['Resultado_Previsto'] = novas_previsoes\n",
        "    df_para_prever['Probabilidade_Prevista'] = novas_probabilidades\n",
        "\n",
        "    print(\"Previsões concluídas e adicionadas ao DataFrame.\")\n",
        "    print(df_para_prever[['Gravidez', 'Glicose', 'IMC', 'Idade', 'Resultado_Previsto', 'Probabilidade_Prevista']].head())\n",
        "\n",
        "else:\n",
        "    print(\"Nenhuma previsão a ser feita.\")"
      ],
      "id": "new_code_2"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 14. Salvar o modelo e os resultados para o Power BI\n",
        "# ===============================================================\n",
        "\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Iniciando preparação dos dados unificados...\")\n",
        "\n",
        "# 1. Preparar o df_treino (Dados Históricos)\n",
        "df_final_treino = df_treino.copy()\n",
        "# Renomeia 'Resultado' para um nome final\n",
        "df_final_treino.rename(columns={'Resultado': 'Resultado_Final'}, inplace=True)\n",
        "# Adiciona uma coluna para identificar a origem do dado\n",
        "df_final_treino['Tipo_Dado'] = 'Histórico'\n",
        "df_final_treino['Probabilidade'] = np.nan # Histórico não tem probabilidade da previsão\n",
        "\n",
        "# 2. Preparar o df_para_prever (Novos Pacientes)\n",
        "df_final_previsao = pd.DataFrame()\n",
        "if not df_para_prever.empty:\n",
        "    df_final_previsao = df_para_prever.copy()\n",
        "    # Renomeia colunas para bater com o df_final_treino\n",
        "    df_final_previsao.rename(columns={\n",
        "        'Resultado_Previsto': 'Resultado_Final',\n",
        "        'Probabilidade_Prevista': 'Probabilidade'\n",
        "    }, inplace=True)\n",
        "    df_final_previsao['Tipo_Dado'] = 'Previsão'\n",
        "\n",
        "# 3. Juntar (Concatenar) os dois DataFrames\n",
        "# Assegurar que ambos tenham as mesmas colunas antes de concatenar\n",
        "colunas_finais = ['Gravidez', 'Glicose', 'PressaoArterial', 'EspessuraPele',\n",
        "                  'Insulina', 'IMC', 'FuncaoGeneticaDiabetes', 'Idade',\n",
        "                  'Resultado_Final', 'Tipo_Dado', 'Probabilidade']\n",
        "\n",
        "# Reordenar colunas (e preencher com NaN se faltar alguma)\n",
        "df_final_treino = df_final_treino.reindex(columns=colunas_finais)\n",
        "if not df_final_previsao.empty:\n",
        "    df_final_previsao = df_final_previsao.reindex(columns=colunas_finais)\n",
        "\n",
        "df_publicar = pd.concat([df_final_treino, df_final_previsao], ignore_index=True)\n",
        "\n",
        "# 4. Limpar o DataFrame para o Google Sheets (gspread não gosta de NaT/NaN)\n",
        "df_publicar = df_publicar.fillna(\"\") # Substitui NaN por string vazia\n",
        "\n",
        "print(f\"Dados unificados criados. Total de linhas: {len(df_publicar)}\")\n",
        "\n",
        "# 5. Salvar o DataFrame de volta na planilha\n",
        "NOME_ABA_POWER_BI = \"PowerBI_Unificado\"\n",
        "\n",
        "try:\n",
        "    print(f\"Tentando acessar a aba '{NOME_ABA_POWER_BI}'...\")\n",
        "    # Tenta encontrar a aba\n",
        "    worksheet = planilha.worksheet(NOME_ABA_POWER_BI)\n",
        "    print(\"Aba encontrada. Limpando dados antigos...\")\n",
        "    worksheet.clear() # Limpa tudo antes de escrever\n",
        "\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    print(\"Aba não encontrada. Criando uma nova...\")\n",
        "    # Cria a aba se ela não existir\n",
        "    worksheet = planilha.add_worksheet(title=NOME_ABA_POWER_BI, rows=10, cols=10) # Tamanho inicial pequeno\n",
        "\n",
        "# 6. Escrever os dados na aba\n",
        "print(f\"Escrevendo {len(df_publicar)} linhas na aba '{NOME_ABA_POWER_BI}'...\")\n",
        "gd.set_with_dataframe(worksheet, df_publicar, include_index=False, include_column_header=True, resize=True)\n",
        "\n",
        "print(\"\\n--- SUCESSO! ---\")\n",
        "print(f\"Seu dashboard unificado está pronto na aba '{NOME_ABA_POWER_BI}' da sua planilha.\")\n",
        "print(\"Agora, conecte o Power BI diretamente nesta aba.\")\n",
        "\n",
        "# 7. (Opcional) Download dos modelos\n",
        "pickle.dump(rf, open('diabetes_model_rf.pkl', 'wb'))\n",
        "pickle.dump(imputer, open('diabetes_imputer.pkl', 'wb'))\n",
        "importancias.to_csv('importancia_variaveis.csv', index=False)\n",
        "files.download('diabetes_model_rf.pkl')\n",
        "files.download('diabetes_imputer.pkl')\n",
        "files.download('importancia_variaveis.csv')\n"
      ],
      "metadata": {
        "id": "JZpm13-FXdE9"
      },
      "id": "JZpm13-FXdE9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}